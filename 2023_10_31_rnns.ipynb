{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/JackXZH/madlibs-starter/blob/main/2023_10_31_rnns.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Import the packages we're going to use\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "import requests"
      ],
      "metadata": {
        "id": "X9fI2hDJWWZj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class CharRNN(nn.Module):\n",
        "    \"\"\" Define a character-level RNN\n",
        "    \"\"\"\n",
        "    def __init__(self, vocab_size, hidden_size):\n",
        "        # Initialize the parent class\n",
        "        super(CharRNN, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        self.vocab_size = vocab_size\n",
        "        # Define an RNN layer\n",
        "        self.rnn = nn.RNN(vocab_size, hidden_size, batch_first=True)\n",
        "        # Define a fully connected layer\n",
        "        self.fc = nn.Linear(hidden_size, vocab_size)\n",
        "\n",
        "    def forward(self, x, hidden):\n",
        "        # Convert characters to one-hot encoding and set type to float\n",
        "        x = nn.functional.one_hot(x, self.vocab_size).float()\n",
        "        # Pass the input and hidden state through the RNN layer\n",
        "        out, hidden = self.rnn(x, hidden)\n",
        "        # Pass the RNN output through the fully connected layer\n",
        "        out = self.fc(out)\n",
        "        return out, hidden"
      ],
      "metadata": {
        "id": "IkYW_yzWXmuo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_text(model, char_to_int, int_to_char, device, start='', length=100):\n",
        "    \"\"\" Run a forward pass of the provided RNN (that is, do \"inference\")\n",
        "    \"\"\"\n",
        "    # Set the model to evaluation mode\n",
        "    model.eval()\n",
        "    # If no start string is provided, randomly select a start character\n",
        "    if not start:\n",
        "        start = np.random.choice(list(char_to_int.keys()))\n",
        "    # Initialize the output string and the hidden state\n",
        "    output, hidden = start, torch.zeros(1, 1, model.hidden_size).to(device)\n",
        "\n",
        "    for i in range(length):\n",
        "        # Convert the last character of output to a tensor\n",
        "        inp = torch.LongTensor([[char_to_int[output[-1]]]]).to(device)\n",
        "        # Generate prediction and update hidden state\n",
        "        pred, hidden = model(inp, hidden)\n",
        "        # Apply softmax to get probabilities\n",
        "        prob = nn.functional.softmax(pred[0][0], dim=0)\n",
        "        # Sample a character from the probability distribution\n",
        "        next_char = int_to_char[torch.multinomial(prob, 1).item()]\n",
        "        # Add the sampled character to the output string\n",
        "        output += next_char\n",
        "    return output"
      ],
      "metadata": {
        "id": "zLaaI_FqaO63"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7JfH1I_xVlCN"
      },
      "outputs": [],
      "source": [
        "# Download the tiny Shakespeare dataset\n",
        "url = \"https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\"\n",
        "response = requests.get(url)\n",
        "# Store the text data as a string\n",
        "text = response.text"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a set of unique characters\n",
        "chars = set(text)\n",
        "# Create a character to index mapping\n",
        "char_to_int = {c: i for i, c in enumerate(chars)}\n",
        "# Create an index to character mapping\n",
        "int_to_char = {i: c for i, c in enumerate(chars)}\n",
        "\n",
        "# Hyperparameters\n",
        "seq_length = 50\n",
        "hidden_size = 128\n",
        "vocab_size = len(chars)\n",
        "# Convert the text data to integer format using the char_to_int mapping\n",
        "text_int = np.array([char_to_int[c] for c in text])\n",
        "\n",
        "# Setting the device to CPU\n",
        "device = torch.device('cpu')\n",
        "\n",
        "# Initialize the model, and set it to use the CPU\n",
        "model = CharRNN(vocab_size, hidden_size)\n",
        "model.to(device)\n",
        "\n",
        "# Define the loss function and the optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "# Convert the integer text data to a PyTorch tensor\n",
        "text_int_tensor = torch.LongTensor(text_int).to(device)\n",
        "# Initialize the hidden state\n",
        "hidden = torch.zeros(1, 1, hidden_size).to(device)"
      ],
      "metadata": {
        "collapsed": true,
        "id": "Pfvm9sOvWflc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Training loop\n",
        "for epoch in range(10):\n",
        "    print(f\"Epoch {epoch+1}\")\n",
        "    # Loop through the text data in chunks of size seq_length\n",
        "    for i in range(0, len(text_int) - seq_length, seq_length):\n",
        "        # Prepare input and target sequences\n",
        "        inputs = text_int_tensor[i:i+seq_length].unsqueeze(0)\n",
        "        targets = text_int_tensor[i+1:i+seq_length+1].unsqueeze(0)\n",
        "\n",
        "        # Zero the gradients\n",
        "        optimizer.zero_grad()\n",
        "        # Forward pass\n",
        "        outputs, hidden = model(inputs, hidden.detach())\n",
        "        # Compute the loss\n",
        "        loss = criterion(outputs.squeeze(), targets.squeeze())\n",
        "        # Backward pass\n",
        "        loss.backward()\n",
        "        # Update the weights\n",
        "        optimizer.step()\n",
        "\n",
        "        # Print training progress and generate text every 10000 steps\n",
        "        if i % 10000 == 0:\n",
        "            print(f\"Epoch {epoch+1}, step {i}, Loss: {loss.item()}\")\n",
        "            print(generate_text(model, char_to_int, int_to_char, device, start='', length=200))\n",
        "            print(\"-\" * 50)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "3RIvLe21WPHw",
        "outputId": "adb8b560-0b93-46e8-80df-16cc192519a1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1\n",
            "Epoch 1, step 0, Loss: 4.184510231018066\n",
            "xJ'\n",
            "oAwkY,KoUyHzISUA,bUVqZdRIbjs HHhVyfa,sBWgiWoY.m?ISWoNNmzi&Xtdy,sX!doE.-.dOEaIKPc,tE?tD'qk\n",
            "oDkgjmIERoCOLW?lGO.hbfYCG mUL:UPqtGMtSpq:AtoPTzRWhjhmUClZr3lPxKnoCN,iM.g&3lIPnAmXORNCkHQV LLzIgmvZXNPse\n",
            "T-q\n",
            "--------------------------------------------------\n",
            "Epoch 1, step 10000, Loss: 3.015928030014038\n",
            "xDInoe nN\n",
            "ikthhriF coeanh ea uns TehetgelceSttyn  aosyvslhwk ,teneium'lyi dyneofheteurdo ouen er guneltorene\n",
            "tto r\n",
            "sn theeune hU?s'tFl siSt ' hwt mot d 'aeeetsnogtr Tladkd\n",
            "esihhmMeilwnsikt i :eose ! md\n",
            "--------------------------------------------------\n",
            "Epoch 1, step 20000, Loss: 2.5597426891326904\n",
            "j, uoee:d\n",
            "nAdyd:t Iad yr y u 'd gitk \n",
            "re sFs,\n",
            " burEIAI oaydenIeruss tblt ,Iy\n",
            "'su'rI.\n",
            "s\n",
            "ISLoXned\n",
            "\n",
            "lVlarvh ur \n",
            "  CoM. ,s wounii , \n",
            "i rairhi hent n sn L y ur irs,\n",
            "\n",
            "\n",
            "V&xZea; b,mcIsot  mI.fc che!r\n",
            "eogo\n",
            "s or\n",
            "--------------------------------------------------\n",
            "Epoch 1, step 30000, Loss: 2.4646565914154053\n",
            "m!\n",
            "!\n",
            "KoLe Gy c\n",
            "Mleuem t.tldodd, Phi\n",
            "s;\n",
            "wer uhe da' waheu,rhisss dhin nI teu,s, \n",
            "hirasd yhtd mad klist theldhshl\n",
            "wU bonhthevg\n",
            " Oink e chated fn ys we limed minv siirmasege s th thid fou! t tharethirech \n",
            "--------------------------------------------------\n",
            "Epoch 1, step 40000, Loss: 1.7807317972183228\n",
            "-Ccwim yir.\n",
            "ZaFl vorckp\n",
            "pomcpppirc, Hhand.\n",
            "\n",
            "WMOVNbUod wgepe wmn\n",
            "fof:\n",
            "yh RouIe\n",
            " mokghand to\n",
            "wouN,\n",
            "IoI ter anandens mobs anduys sharhesponi;\n",
            "Thetire ber pow eods ykoy.\n",
            "\n",
            "B!NIIUS:\n",
            "Ao f!uige mere\n",
            "nomb twit \n",
            "--------------------------------------------------\n",
            "Epoch 1, step 50000, Loss: 2.353260040283203\n",
            "wRTame dast a.c.\n",
            "bomanh hinive seirbe pagb torblr ly ceed ass afritl an tave ousD aol\n",
            "rS romt, be fore!f ial\n",
            "ro con foy aase\n",
            "Mave vave\n",
            "ditharve spesle ihas to nur ase nonw.r\n",
            "YOq fear, ds mesnet dom yov\n",
            "--------------------------------------------------\n",
            "Epoch 1, step 60000, Loss: 2.5061328411102295\n",
            "iggy ?o\n",
            " inlbr arozs reser\n",
            "cEHils Nidthe\n",
            "d ingatu, ro e Lchitzdipe terOs, Me le gseoos out hham'l.\n",
            "\n",
            "CMceNINSUS:\n",
            "Whir\n",
            "Bybs!\n",
            "bos tile, dot an IUY, he!n soulc\n",
            "-oeery to p at ne irdtes:\n",
            "You sMrihe , ther :\n",
            "--------------------------------------------------\n",
            "Epoch 1, step 70000, Loss: 2.287181854248047\n",
            "bunke then.\n",
            "\n",
            "OCOOOLIANUL:\n",
            "We 'runvet he lasy, Con andi'it rot Ofeclepes yotU oatt\n",
            "We, shor misham mand wo.\n",
            "\n",
            "BRCIULA-\n",
            "\n",
            "IOGINUS:\n",
            "Toul ano thes wome toce coicerre me cur Ounsruveines\n",
            "Dgay f thall re foppa\n",
            "--------------------------------------------------\n",
            "Epoch 1, step 80000, Loss: 1.8516950607299805\n",
            "S:\n",
            "I hanthue hod hoid of'gany see hares:\n",
            "H venretow t teur\n",
            "\n",
            "LO leyr' tis with, softYous.\n",
            "\n",
            "ORUANUS:\n",
            "Nirer:\n",
            "The nrtiniged hhers'ew!\n",
            "\n",
            "FThesure\n",
            "An gitg nof peol!\n",
            "\n",
            "CarIzthe Ronlathe when a'p yod thes\n",
            "Fown a\n",
            "--------------------------------------------------\n",
            "Epoch 1, step 90000, Loss: 2.213552474975586\n",
            "SIB: irs\n",
            "Art\n",
            "\n",
            "Tuthrs mongert dim to thes, Iilp ase:\n",
            "Iis aro my,ever\n",
            "Be theareDne Vy ald I chasenges ia tong, you he init Bemenmce foun, st ho me ichinse mo noby ponienC\n",
            "Io the tolrttiy boor.\n",
            "\n",
            "MORIONUS:\n",
            "--------------------------------------------------\n",
            "Epoch 1, step 100000, Loss: 1.9228674173355103\n",
            "y aUr make, frtut ifongll is\n",
            "Thoy is pustr om med the we peam he tits lomsgoins oon hy.\n",
            "Yous: be Phey whes shourd pfote.\n",
            "\n",
            "MENENIUS:\n",
            "Whe, I myou wsicet ath rou ge wyour er brterst se muen\n",
            "swalr breat:\n",
            "M\n",
            "--------------------------------------------------\n",
            "Epoch 1, step 110000, Loss: 2.398332357406616\n",
            "in.\n",
            "ICORIKUS:\n",
            "Nhat have oplome\n",
            "OMUOUS:\n",
            "Sheking an lart coulinakstA lard mure ohertouf mwereams, fur morgegwt llestrald; fath hore far io that ho sac?\n",
            "Whou me the choues indo poot, an wimy yole, I tampt\n",
            "--------------------------------------------------\n",
            "Epoch 1, step 120000, Loss: 2.1621198654174805\n",
            "thet ingrss the keis as, veno the, reris my poUk:\n",
            "Batir hir coplath'd.\n",
            "\n",
            "SICINIUS:\n",
            "Whutr inatt wily aipsse fhar id; los Rave dere Oole hort. bist; penilu's lepsesd whice li.\n",
            "\n",
            "SICINIUS:\n",
            "Whie llecremurs o\n",
            "--------------------------------------------------\n",
            "Epoch 1, step 130000, Loss: 1.5865997076034546\n",
            "DTpyoughi:\n",
            "Hl in meshe tou thes hikld, Your Roarcibest,\n",
            "I'd orelle. Ro hatl. Vhy tnengwoun?\n",
            "Tark thas fear warc't he aid for the\n",
            "Pr totr forroun sousrist he peathe by your pead ae heerr, :pastole e't; \n",
            "--------------------------------------------------\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-17-42e706df1885>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0;31m# Backward pass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m         \u001b[0;31m# Update the weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    490\u001b[0m                 \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    491\u001b[0m             )\n\u001b[0;32m--> 492\u001b[0;31m         torch.autograd.backward(\n\u001b[0m\u001b[1;32m    493\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    494\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    249\u001b[0m     \u001b[0;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    250\u001b[0m     \u001b[0;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 251\u001b[0;31m     Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    252\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    253\u001b[0m         \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    }
  ]
}